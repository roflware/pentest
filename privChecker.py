#!/usr/bin/python

# Todo: Place in try and except when URLs are down or were incorrectly parsed

"""
About:  Parse all links from file, and it will generate an HTTP Status code
		based upon what cookie you supply the script.
Author: Roflware

"""

import sys, urllib, urllib2, urlparse, httplib2
from BeautifulSoup import BeautifulSoup

def process(url):

	# Let's open up a temporary file to store all of the links from our page
	f = open('links.txt', 'w')

	# We process the URL at this stage after asking for the cookie file where we are grabbing the links
	starterCookieFile = raw_input("Please supply the cookie file you wish to use to grab the initial pages: ")
	with open(starterCookieFile) as m:
		suppliedCookie = m.read()

	customSuppliedCookie = {'Cookie': '%s' % (suppliedCookie)}

	# Here we take our user-submitted cookie for the link parsing and feed it to the url request
	req = urllib2.Request(url, None, customSuppliedCookie)
	page = urllib2.urlopen(req)

	text = page.read()
	page.close()

	soup = BeautifulSoup(text)

	# Here we use Beautiful Soup's built in tricks: all we have to do is tell it to find all 'a' anchors with href's and print it out accordingly
	for tag in soup.findAll('a', href=True):
		tag['href'] = urlparse.urljoin(url, tag['href'])
		# We then write the URLs to the previously opened file
		f.write(tag['href'] + '\n')
	f.close()
	
	# Start the next process of verification where we open our links.txt file up again
	with open("links.txt") as h:
		links = h.read().splitlines()

	# The user provides a cookie file which contains everything after the "Cookie:" field and read it into a list
	laterCookieFile = raw_input("Please supply the cookie file you wish to use: ")
	with open(laterCookieFile) as g:
		cookie = g.read()

	print "\n"
	print "======================================================"
	print "Cookie: %s" % (cookie)
	print "======================================================"
	print "\n"

	print "======================================================"
	print "Results:"
	
	# We now loop through the list links with our iterator, s, then feed a custom header (the cookie) into every single request
	for s in links:
		http = httplib2.Http()
		customHeader = {'Cookie': '%s' % (cookie)}
		h = httplib2.Http(".cache")
		response, content = h.request(s, "HEAD", headers=customHeader)

		# Here we check what HTTP status code is returned per web page
		if response.status==200:
			print "%s = 200, Page Found" % (s)
		elif response.status==404:
			print "%s = 404, Page Not Found" % (s)
		elif response.status==401:
			print "%s = 401 Unauthorized" % (s)
		elif response.status==403:
			print "%s = 403 Forbidden" % (s)

	print "======================================================"
	print "\n"
# process(url)

def main():
    if len(sys.argv) == 1:
        print "Link Extractor"
        print "Usage: %s URL [URL]..." % sys.argv[0]
        sys.exit(1)
    # else, if at least one parameter was passed
    for url in sys.argv[1:]:
        process(url)
# main()

#############################################################################

if __name__ == "__main__":
    main()